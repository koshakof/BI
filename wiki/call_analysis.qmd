---
title: 'Как проверить качество входящих звонков из рекламных кампаний, если нет CRM-системы и времени прослушивать каждый звонок'
author: "Александр Коршаков"
categories: [Анализ данных]
date: "2024-03-20"
date-format: long
format:
  html:
    page-layout: article
    toc: true
    toc-location: right
    fig-width: 10
    fig-height: 8
    title-prefix: "ihourglass"
    pagetitle: "Как проверить качество входящих звонков из рекламных кампаний"
    description-meta: "Применяем метод статистической оценки для анализа эффективности входящиз звонков в компанию, если нет CRM-системы."
    author-meta: "Коршаков Александр"
  
execute:
  
  warning: false
  message: false
  error: false
  echo: false
editor_options:
  chunk_output_type: console
---

<!-- Чанк для пред. настроек -->

```{r}
#| label: setup
#| cache: false


# загрузка библиотек 
library(DT)
library(tidyverse)
library(kableExtra)
library(stopwords)
library(tidytext)
library(widyr)
library(ggraph)
library(igraph)
library(scales)
```


```{r}
# Пред. установки

## путь загрузки данных
file_path <- '~/Hourglass/wiki/data/'
```


<!-- END -->



## Цель анализа

Главная цель — понять качество входящих звонков из платной выдачи поисковых систем (рекламные кампании в Я.Директ) без привязки к CRM системе для косвенной оценки эффективности рекламных кампаний.

Основная гипотеза — проведя текстовый анализ мы увидим, с каким интересом (интентом, потребностью, намерением) обращаются клиенты в компанию. Если потребность соответствует деятельности компании, то трафик можно считать релевантным и качественным, а значит рекламные кампании работает правильно, т.е. привлекает целевую аудиторию. Таким образом наша нулевая и альтернативные гипотезы звучат следующим образом:

- $H_0$ — разницы между интентом входящих звонков из платного канала и деятельностью компании нет. Это хороший исход.
- $A$ — разница есть, а значит трафик не релевантный и рекламные кампании привлекают не целевую аудиторию.

:::{.notes}
Важно отметить, что доказывать релевантность платного трафика будем через сравнение с органическим трафиком (трафик из поисковых систем), так как органический трафик считается эталоном по качеству входящих звонков, при условии, что мы принимает тот факт, что SEO оптимизация сайта сделана на высоком уровне. 
:::



## Вводные данные

- Рекламная кампания дается в поисковой системе Яндекс по направлению продажи тракторов.
- Анализируемый период с 10.03.2023 по 12.04.2023.
- За период зафиксировано $369$ звонков с платных и органических каналов.
- Файл выгрузка с транскрибацией входящих звонков содержит $14\ 623$ строк.




## Загрузка и подготовка данных

Загрузим таблицу с данными, которая содержит транскрибацию входящих звонков.


```{r}
# загрузка данных по клиенту
df <- data.table::fread(
  file = paste0(file_path, 'text_df.csv')
  ) |> 
  select(-Звонивший)
```


```{r}
datatable(
  df[100:104,],
  rownames = FALSE,
  class   = 'compact nowrap',
  options = list(
    pageLength = 10,
    dom        = 'Btpli',
    language   = list(url = 'https:://cdn.datatables.net/plug-ins/1.13.6/i18n/ru.json')
    )
  )
```


<br/>

Далее преобразуем таблицу для дальнейшего анализа. 

- уберем стоп-слова и служебные части речи
- проведем токенизацию слов для дальнейшего анализа
- приведем столбцы в нужный формат

```{r}
# подключаем список стоп слов
stop_ru <- tibble(Слово = stopwords("ru", source = "stopwords-iso"))
```


```{r}
# обработка данных
text_df <- df  |>  
  unnest_tokens(Слово, Фраза) |> 
  filter(!str_detect(Слово, "\\W|\\d"))  |>  
  anti_join(stop_ru, by = 'Слово')  |> 
  mutate(
    Дата = as.Date(str_extract(Дата, "\\d+\\-.+")),
    Неделя = clock::date_format(Дата, format = "%W %b"),
    День = clock::date_format(Дата, format = "%w %a"),
    ID = as.character(`ID звонка`),
    N = row_number()
    )
```


```{r}
datatable(
  text_df[100:104,],
  rownames = FALSE,
  class   = 'compact nowrap',
  options = list(
    pageLength = 10,
    dom        = 'Btpli',
    language   = list(url = 'https:://cdn.datatables.net/plug-ins/1.13.6/i18n/ru.json')
    )
  )
```

<br/>

В итоге получили фрейм данных в котором фразы были разбиты на слова, при этом очищенный от стоп слов (предлоги, союзы, знаки препинания и т.п.)




## Анализ активности во времени по кол-ву слов

Рассмотрим кол-во слов распределенных на временном промежутке.


>**График: Активность по дням**

```{r}
text_df  |>  
  count(Дата) |> 
  ggplot(
    aes(
      x = Дата, 
      y = n
      ))+
  geom_col() +
  geom_smooth(
    aes(
      x = Дата, 
      y = n
      ),
    method = "lm",
    se = FALSE
    )+
   geom_smooth(
     aes(
       x=Дата,
       y = n
       ), 
     method = "loess",
     se = FALSE,
     color = "red",
     linetype = 2,
     size = 0.1
     )+
  ggpubr::stat_cor(
    #label.x = 8,
    label.y.npc = "top",
    label.sep = "\n",
    color = "gray40",
    size = 3
    )+
  theme_minimal()+
  labs(
    x = NULL,
    y = "Кол-во слов"
    )+
  theme(
    axis.title = element_text(size = 10, color = "gray40")
  )
```


Стат данные к графику.

```{r}
text_df |> 
  select(Дата) |> 
  count(Дата) |> 
  summarise(mn    = round(mean(n)),
            md    = round(median(n)),
            sd    = round(sd(n)),
            se    = round(sd(n)/sqrt(length(n))),
            max   = max(n),
            min   = min(n),
            delta = max-min,
            total = sum(n)) |> 
  kbl() |>
  kable_paper(lightable_options = c("hover", "condensed", "responsive"))
```


<br/>

>**График: Активность по неделям**

```{r}
text_df |>
  count(
    Неделя, 
    sort = TRUE, 
    name = "fq"
    ) |>
  mutate(
    hl = case_when(
      fq > 6000 ~ "A",
      TRUE ~ "B"
    ),
    lb = case_when(fq > 6000 ~ fq)
  ) |>
  ggplot(aes(x = Неделя, y = fq, fill = hl)) +
  geom_col(show.legend = FALSE) +
  scale_fill_manual(
    values = c(
      A = "dodgerblue", 
      B = "gray49"
      )) +
  geom_text(aes(label = lb),
    position = position_stack(vjust = 0.95),
    color    = "white",
    size     = 3.5
  ) +
  theme_minimal() +
  labs(
    x = NULL,
    y = "Кол-во слов"
  ) +
  theme(
    axis.title = element_text(size = 10, color = "gray40"),
    panel.grid = element_blank()
  )
```


**Вывод**

На графике видим волнообразную структуру данных, которая согласуется с днями недели. Виден четкий тренд на увеличение кол-ва слов (синяя линия), об этом также свидетельствует положительный коэффициент корреляции $R=0.14$. Это может говорить о том, что кол-во звонков со временем поступает больше. 

Также по красной линии тренда видно, что пик по кол-ву слов пришёлся на март и на вторую неделю апреля.

В среднем за период в разговоре и у клиента и у оператора употребляется от $220$ до $1\ 776$ слов, в среднем $998$. Всего за анализируемый период было произнесено почти $41\ 000$ слов

Что касается дней недели. Самые активные недели были $11$ и $12$ недели марта. При этом отметим что $13$ неделя апреля пришлась на субботу и воскресенье, а так как в эти дни активности почти нет, то на графике также их не будет.

Рассмотрим самые активные дни недели с точки зрения использования слов.


> **График: Самые активные дни недели**

```{r}
text_df |> 
  count(День, sort = TRUE, name = "fq") |>
  mutate(hl = case_when(fq > 8000 ~ "A",
                        TRUE      ~ "B"),
         lb = case_when(fq > 8000 ~ fq)) |> 
  ggplot(aes(x = День, y = fq, fill = hl)) +
  geom_col(show.legend = FALSE)+
  scale_fill_manual(values = c(A = "dodgerblue", B = "gray49")) +
  geom_text(aes(label = lb), 
            position = position_stack(vjust = 0.95),
            color    = "white", 
            size     = 3.5) +
  theme_minimal() +
  labs(x = NULL,
       y = "Кол-во слов") +
  theme(axis.title = element_text(size = 10, family = "roboto", color = "gray40"),
        panel.grid = element_blank())
```


**Вывод**

Самые активные дни недели с точки зрения употребления слов со среды по пятницу. Что согласуется с кол-вом поступающих звонков на эти дни. Самые не активные дни суббота и воскресенье.


<br/>




## Сегментный анализ

Рассмотрим кол-во употребляемых слов в разрезе «оператор и клиент».


>**График: сегменты в разрезе оператор / клиент**

```{r}

# график в разрезе оператор клиент
text_df |>
  count(Канал) |>
  ggplot(mapping = aes(x = Канал, y = n)) +
  geom_col(aes(fill = Канал), show.legend = FALSE) +
  scale_fill_manual(values = c("gray40", "dodgerblue")) +
  geom_text(
    aes(label = n), 
    position = position_stack(vjust = 0.93), 
    color = "white", size = 3.5
    ) +
  theme_minimal() +
  labs(
    x = NULL,
    y = "Кол-во слов"
  ) +
  theme(
    axis.title = element_text(size = 10, family = "roboto", color = "gray40"),
    panel.grid = element_blank()
  )
```


**Вывод**

На графике видно, что оператор использует почти в два раза больше слов, чем клиент. Возможно разница связан с работой автоответчка, а также с использованием операторами уточняющих слов.


<br/>


>**График: Тренд по словам по оператору и клиенту**

```{r}

# тренд по словам по оператору и клиенту
text_df |>
  count(Дата, Канал) |>
  ggplot() +
  geom_bar(aes(x = Дата, y = n, fill = Канал),
    stat = "identity",
    position = "dodge"
  ) +
  geom_smooth(aes(x = Дата, y = n, color = Канал),
    method = "lm",
    se = FALSE,
    show.legend = FALSE
  ) +
  ggpubr::stat_cor(
    mapping = aes(x = Дата, y = n, fill = Канал),
    label.y.npc = "top",
    label.sep = "\n",
    color = "gray40",
    size = 3
  ) +
  theme_minimal() +
  labs(
    x = NULL,
    y = "Кол-во слов"
  ) +
  theme(
    legend.position = "top",
    legend.text = element_text(size = 10, color = "gray40"),
    legend.title = element_blank(),
    axis.title = element_text(size = 10, color = "gray40"),
    axis.text = element_text(size = 7),
    panel.grid = element_blank()
  )

```


**Вывод**

Наблюдаем положительный рост по кол-ву слов у оператора. У клиентов кол-во слов практически не изменяется. С учетом роста повторных звонков, возможно, приходится уделять больше внимание клиенту, который находится ближе к этапу сделки.


<br/>


>**График: Кол-во слов в разрезе каналов по параметру «повторный»**

```{r}

text_df |>
  filter(Медиум %in% c("organic", "cpc")) |>
  count(Канал, Медиум, `Первичный звонок`, name = "Слово") |>
  ggplot(mapping = aes(x = Канал, y = Слово, fill = `Первичный звонок`)) +
  geom_col() +
  facet_wrap(~Медиум) +
  scale_fill_manual(values = c("gray40", "dodgerblue")) +
  theme_minimal() +
  labs(
    x = NULL,
    y = "Кол-во звонков"
  ) +
  theme(
    legend.position = "top",
    legend.title = element_text(color = "gray40", size = 10),
    legend.text = element_text(color = "gray40", size = 10),
    axis.title = element_text(size = 10, color = "gray40"),
    strip.text.x = element_text(size = 12, colour = "gray40"),
    panel.grid = element_blank()
  )

```

**Вывод**

Видно, что кол-во слов при повторном и первичном звонке отличается. Клиент и оператор используют больше слов при первичном звонке, при этом это на зависит от канала из которого пришел клиент, пропорция соблюдается.


<br/>




## Анализ частотности слов

Данный анализ покажет какие ключевые слова используются наиболее часто в разговоре в различных срезах. Это даст первое понимание о совпадении интента клиентов с деятельностью компании.

Рассмотрим частотность слов в разрезе оператор клиент.

>**График: Частотность слов в разрезе оператор - клиент**

```{r}

# подготовим график по оператору
oper_pl <- text_df |>
  count(Канал, Слово, sort = TRUE, name = "Частота") |>
  filter(Канал %in% "Оператор") |>
  slice(1:10) |>
  mutate(Слово = reorder(Слово, Частота)) |>
  ggplot(aes(x = Частота, y = Слово)) +
  geom_col() +
  theme_minimal() +
  labs(y = NULL) +
  theme(
    axis.title = element_text(size = 10, color = "gray40"),
    panel.grid.major.y = element_blank()
  )
```


```{r}
# подготовим график по клиенту
client_pl <- text_df |>
  count(Канал, Слово, sort = TRUE, name = "Частота") |>
  filter(Канал %in% "Клиент") |>
  slice(1:10) |>
  mutate(Слово = reorder(Слово, Частота)) |>
  ggplot(aes(x = Частота, y = Слово)) +
  geom_col() +
  theme_minimal() +
  labs(y = NULL) +
  theme(
    axis.title = element_text(size = 10, color = "gray40"),
    panel.grid.major.y = element_blank()
  )
```


```{r}

# совмещаем два графика в один
ggpubr::ggarrange(
  oper_pl, 
  client_pl,
  ncol = 2, nrow = 1,
  labels = c("Оператор", "Клиент"),
  label.x = -0.06,
  label.y = 1.01,
  font.label = list(size = 10, face = "plain", color = "#19a6b3")
)
```


**Вывод**

У оператора мы видим ответ автоответчика, что объясняет почему оператор употребляет в два раза больше слов чем клиент. У клиента видим слова связанные с деятельностью компании. Первый признак совпадения интента.

Сравним ключевые слова у клиентов из органического и платного трафика. Сравниваем именно с органическим трафиком потому что считаем, что из данного канала приходят наиболее релевантные клиенты по отношению к деятельности компании.

Предварительно уберем из данных ряд мусорных слов, которые не несут смысловой нагрузки. В таблице приведен пример таких слов.


>**Таблица: Минус-слова**

```{r}
# загружаем список минус слов из csv файла
## загрузка данных
my_stop <- data.table::fread(
  file = paste0(file_path, 'stop.txt')
  )
```


```{r}
# убираем минус-слова
text_correct <- anti_join(text_df, my_stop, by = "Слово")

my_stop |> 
  slice(1:5) |> 
  kbl() |> 
  kable_paper(
    lightable_options = c(
      "hover", 
      "condensed",
      "responsive"
      ),
    font_size = 12
    )
```


<br/>


>**График: Частотность слов по клиентам в разрезе каналов**

```{r}
# подготовим график по клиентам из органики
org_pl <- text_correct |>
  count(Медиум, Канал, Слово, sort = TRUE, name = "Частота") |>
  filter(Канал %in% "Клиент" & Медиум %in% "organic") |>
  slice(1:10) |>
  mutate(Слово = reorder(Слово, Частота)) |>
  ggplot(aes(x = Частота, y = Слово)) +
  geom_col() +
  theme_minimal() +
  labs(y = NULL) +
  theme(
    axis.title = element_text(size = 10, color = "gray40"),
    panel.grid.major.y = element_blank()
  )
```


```{r}
# подготовим график по клиенту
cpc_pl <- text_correct |>
  count(Медиум, Канал, Слово, sort = TRUE, name = "Частота") |>
  filter(Канал %in% "Клиент" & Медиум %in% "cpc") |>
  slice(1:10) |>
  mutate(Слово = reorder(Слово, Частота)) |>
  ggplot(aes(x = Частота, y = Слово)) +
  geom_col() +
  theme_minimal() +
  labs(y = NULL) +
  theme(
    axis.title = element_text(size = 10, color = "gray40"),
    panel.grid.major.y = element_blank()
  )
```


```{r}
# совмещаем два графика в один
ggpubr::ggarrange(
  org_pl, 
  cpc_pl,
  ncol = 2, nrow = 1,
  labels = c("Organic", "CPC"),
  label.x = -0.06,
  label.y = 1.01,
  font.label = list(size = 10, face = "plain", color = "#19a6b3")
)
```


**Вывод**

Визуально можно увидеть, что по частотности слов у клиентов из органического и платного трафика слова практически совпадают. Интересуют в первую очередь трактора, при этом в платном канале также интересуют погрузчики и оборудование. Клиенты интересуются наличием и запрашивают коммерческое предложение. Также в платном канале интересуются лизингом.


<br/>



## Анализ ассоциированности ключевых слов

Однако, нас интересует не только кол-во слов, но и как они взаимосвязаны между собой. Найдем наиболее соотносимые (ассоциированные) ключевые слова через коэффициент $phi$. Данный Коэффициент эквивалентен корреляции Пирсона.


>**График: Пример ассоциативности ключевых слов**

```{r}
# комбинируем стоп слов
stop_word <- bind_rows(stop_ru, my_stop)
```


```{r}
# создаем фрейм данных
df_section <- df |> 
    filter(
      Канал %in% "Клиент",
      Медиум %in% c("organic", "cpc")
      ) |>
    select(Фраза) |> 
    mutate(section = row_number() %/% 10) |>
    filter(section > 0) |>
    unnest_tokens(word, Фраза) |>
    filter(!word %in% stop_word$Слово)
```


```{r}
# создадим фрейм данных через библиотеку widyr
## кол-во встречающихся пар слов
word_pairs <- df_section|>
  pairwise_count(word, section, sort = TRUE)
```


```{r}
## корреляция между словами
word_cors <- df_section|>
  group_by(word)|>
  filter(n() >= 20)|>
  pairwise_cor(word, section, sort = TRUE)
```


```{r}
## объеденяем даные
wd_cors_pairs <- right_join(word_pairs, word_cors, by=c("item1","item2"))
```


```{r}
# подготовим данные к визуализации
plot_wd_pairs <- wd_cors_pairs |>
  filter(item1 %in% c("интересует", "мтз", "трактор", "предложение", "цена","какие")) |> 
  group_by(item1) |> 
  slice_max(correlation, n = 5) |>
  ungroup()
```


```{r}
#|fig-width: 10
#|ffig-height: 10

# визуализируем
plot_wd_pairs |> 
  ggplot(
    aes(
      reorder_within(
        item2, 
        correlation, 
        item1
        ),
      correlation
      )) +
  geom_bar(stat = "identity") +
  scale_x_reordered(sep = " ") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(
    x = NULL,
    y = "Корреляция"
    ) +
  theme(
    axis.title = element_text(size = 10, color = "gray40"),
    strip.text.x = element_text(size = 12, colour = "gray30"),
    panel.grid.major.y = element_blank()
    )
```


<br/>


На графике мы видим ключевые слова и ассоциированные слова на оси $y$, а также видна сила корреляции по оси $x$. Таким образом можно провести анализ запросов входящих звонков и дополнительно проверить, чем интересуются клиенты. В данном случаи запросы клиентов полностью совпадают с деятельностью компании.

Также можно построить граф, где будут отображены кластеры слов и их взаимосвязь, что позволит взглянуть на картину с еще большего масштаба.


<br/>


>**Граф: Кластеры и ассоциативность между словами**

```{r}
#|fig-width: 10
#|ffig-height: 10

set.seed(42)

wd_cors_pairs |>
  filter(correlation > .06) |>
  graph_from_data_frame() |>
  ggraph(layout = "fr") +
  geom_edge_link(
    aes(
      edge_alpha = correlation, 
      edge_width = correlation
      ),
    edge_colour = "cyan4", show.legend = T
    ) +
  geom_node_point(color = "gray30", size = 3) +
  geom_node_text(
    aes(label = name),
    color = "gray30",
    repel = TRUE
    ) +
  theme_void()
```


**Вывод**

На графе отображены взаимосвязи с силой выше $0,06$ иначе граф получается не разборчив. На графе можно увидеть кластеры и наиболее сильные взаимосвязи. Например, пара «коммерческое предложение» соединена темной толстой линией. Это означает сильную ассоциативную взаимосвязь между словами. 

Используя графы и таблицы можно найти главные интересы клиентов, что будет полезно для маркетологов, менеджеров по продажам и трафик менеджеров.


<br/>




## Корреляционный анализ каналов

Проведем корреляционный анализ, чтобы убедиться наверняка с точки зрения статистики, что запросы у клиентов из разных каналов совпадают.


>**График: Сравнение частотности слов у клиентов в разрезе каналов**

```{r}
# подготовим данные для построения графика
frequency <- text_correct  |> 
  filter(Канал %in% "Клиент" & Медиум %in% c("organic", "cpc")) |>
  count(Медиум, Слово) |>
  group_by(Медиум) |>
  mutate(proportion = n / sum(n)) |>
  select(-n) |>
  pivot_wider(names_from = Медиум, values_from = proportion)
```


```{r}
#| fig-width: 10
#| fig-height: 10

# построим график
ggplot(
  frequency, 
  aes(
    x = organic, 
    y = cpc,
    color = abs(cpc - organic)
    )) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = Слово), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(
    limits = c(0, 0.001),
    low = "darkslategray4", high = "gray75"
  ) +
  theme_minimal() +
  labs(
    y = "CPC", 
    x = "Organic"
    ) +
  theme(
    legend.position = "none",
    axis.title = element_text(size = 10, color = "gray40", family = "roboto"),
    panel.grid = element_blank()
  )

```


**Вывод**

Слова, близкие к пунктирной линии на графиках, имеют сходные частоты в обоих наборах текстов, в то время как те, которые находятся далеко от линии, встречаются чаще в одном наборе текстов, чем в другом.

Например, слова «банк», «инн», «краснодаре», «погрузчик» встречаются чаще в платном канале (СРС), тогда как слова «доставка», «беларус», «вал» встречаются чаще в органическом трафике.

Слова которые употребляются в обоих каналах часто «трактор», «восемьдесят», «коммерческое», «предложение».

Обратим внимание, что множество слов распространяются на более низкие частоты (облако точек сосредоточено в нижнем левом углу графика). Эта характеристика указывают на то, что клиенты из обеих каналов используют большой объём низкочастотных слов при общении. 

Количественно оценим, насколько похожи и различаются эти наборы частот слов, используя корреляционный тест. 

```{r}
test <- cor.test(x = frequency$cpc, y = frequency$organic)
```

Значение корреляции

```{r}
paste0("corr: ", test$estimate)

```

Значение p-value

```{r}
paste0("p-value: ", test$p.value)
```


**Вывод**

Тест показывает, что частота употребления слов сильно коррелирует между каналами. На это указывает коэффициент корреляции $cor≈0,9$ (при норме $0,6$ и чем ближе к $1$, тем выше корреляция). При этом уровень достоверности теста высокий и явно ниже $0,05$ на это указывает $p-value$ с $16$ знаками после запятой. Т.е. клиенты из данных каналов используют сопоставимый по объему и смыслу слова.


Также мы можем доказать схожесть набора текстовых данных через Закон Ципфа.


<br/>




## Закон Ципфа

Закон Ципфа (Zipf’s law) – имперический закон, согласно которому частота появления слова обратно пропорциональна его рангу. Т.е. чем реже встречается слово в наборе данных, тем выше ранг. Вычисляете по формуле:

$$freq(r)=A \times N \times r^{-1}$$

- $freq(r)$ – частотность слова с рангом $r$
- $r$ — ранг слова в списке слов, упорядоченных по убыванию частоты
- $N$ – общее количество слов или символов в тексте
- $A$ – константа, которая зависит от конкретного текста и языка, обычно $0.1$

Формула говорит, что если ранг слова увеличивается в два раза, то его частота встречаемости уменьшается в два раза. С другой стороны, чем меньше ранг слова, тем выше его частота встречаемости.

Рассмотрим как распределились слова по частотности в разрезе каналов (переменная $TF$ по оси $x$).

<br/>


>**График: Распределение частотности слов в разрезе каналов**

```{r}
# создадим фрейм данных
zip_df <- df %>% 
  unnest_tokens(Слово, Фраза) %>% 
  filter(!str_detect(Слово, "\\W|\\d")) %>% 
  filter(Канал %in% "Клиент" & Медиум %in% c("organic","cpc")) %>% 
  count(Медиум, Слово, sort = TRUE)
```


```{r}
total_words <- zip_df %>% 
  summarise(total = sum(n), .by = Медиум)
```


```{r}
zip_df <- left_join(zip_df, total_words)
```


```{r}
# визуализируем частоту слов (TF)
ggplot(zip_df, aes(n / total, fill = Медиум)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.008) +
  facet_wrap(~Медиум, ncol = 2, scales = "free_y") +
  labs(
    x = "TF",
    y = "Частота употребления слова"
  ) +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 10, color = "gray40"),
    strip.text.x = element_text(size = 12, colour = "gray40"),
    panel.grid = element_blank()
  )

```


**Вывод**

На графике, в обоих наборах данных видим длинные хвосты т.е. чрезвычайно редкие слова. График демонстрирует схожее распределение для каналов, с большим количеством слов, которые встречаются редко, и меньшим количеством слов, которые встречаются часто.

Закон Ципфа визуализируется путем построения ранга по оси $x$ и частоты членов по оси $y$ в логарифмических масштабах. При построении графика таким образом, обратно пропорциональная зависимость будет иметь постоянный отрицательный наклон.

<br/>


>**График: Закон Ципфа**

```{r}
# добавим ранг в данные
freq_by_rank <- zip_df %>% 
  group_by(Медиум) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total) %>%
  ungroup()
```


```{r}
# визуализируем закон ципфа
freq_by_rank %>%
  ggplot(aes(rank, `term frequency`, color = Медиум)) +
  geom_abline(
    intercept = -0.39, slope = -1.2,
    color = "gray50", linetype = 2
  ) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = T) +
  scale_x_log10() +
  scale_y_log10() +
  theme_minimal() +
  labs(
    x = "Ранг",
    y = "TF"
  ) +
  theme(
    legend.position = "top",
    legend.title = element_text(color = "gray40", size = 10),
    legend.text = element_text(color = "gray40", size = 10),
    axis.title = element_text(size = 10, color = "gray40"),
    strip.text.x = element_text(size = 12, colour = "gray40"),
    panel.grid = element_blank()
  )

```


**Вывод**

Видим на графике, что оба текстовых набора данных по каналам очень похожи между собой и что зависимость между рангом и частотой имеет отрицательный наклон (чем выше ранг, тем ниже частотность $TF$). 

Особое внимание на пунктирную линию. Это значение степенного закона средней части набора данных для слов с рангом от $10$ до $500$. Мы видим, что основная масса слов лежит на пунктирной линии, при этом обе линии накладываются друг на друга. Что говорит о высокой схожести набора данных. Это доказывает, что обращения из платного канала схожи с обращениями из органической выдачи.


<br/>


## Вывод по текстовому анализу

С помощью двух методов анализа (корреляционный анализ и закон Ципфа), мы доказали схожесть текстовых набора данных между платным и органическим каналами. Таким образом принимаем нашу нулевую гипотезу о том, что разницы между интеном во входящих звонках из платного канала и входящих звонках из органического трафика НЕТ. Т.е. клиенты из обеих каналов обращаются в компанию с одними и теми же интересами.

Принимаем гипотезу:

- H0 — разницы между интентом входящих звонков из платного канала и деятельностью компании нет.

Данный анализ применим и для других источников трафика, главное условие это наличие качественного органического трафика, либо набора ключевых слов (семантическое ядро), которые описывают деятельность компании.